#!/bin/bash

# bail out if anything fails
set -e

###################
##  BUILD STEPS  ##
###################

DOCKER_USER=shravspy

create_scale_app () {
  cd app/flask
  docker build -t ${DOCKER_USER}/scale-app .
}

create_spark_base () {
  cd app/spark/spark-base
  docker build -t ${DOCKER_USER}/spark-base .
}

create_from_spark_base () {
  cd app/spark/spark-master
  docker build -t ${DOCKER_USER}/spark-master .
  cd ../spark-worker
  docker build -t ${DOCKER_USER}/spark-worker .
  cd ../spark-client
  docker build -t ${DOCKER_USER}/spark-client .
}

push_to_docker_hub () {
  docker login
  # docker push ${DOCKER_USER}/scale-app
  docker push ${DOCKER_USER}/spark-base
  docker push ${DOCKER_USER}/spark-master
  docker push ${DOCKER_USER}/spark-worker
  docker push ${DOCKER_USER}/spark-client
}

cleanup_images () {
  # docker rmi ${DOCKER_USER}/scale-app
  docker rmi ${DOCKER_USER}/spark-base
  docker rmi ${DOCKER_USER}/spark-master
  docker rmi ${DOCKER_USER}/spark-worker
  docker rmi ${DOCKER_USER}/spark-client
}

gen_pg_assets () {
  # generate hash_names.csv w/ md5_to_paths.json as intermediate
  cd app/postgres
  curl http://hog.ee.columbia.edu/craffel/lmd/md5_to_paths.json > md5_to_paths.json
  rm -rf assets/hash_names.csv
  python3 create_midi_instrument_csv.py
  rm -rf md5_to_paths.json
}

#########################
##  START APPLICATION  ##
#########################


setup_scale_app () {
  # setup secrets
  #kubectl apply --recursive -f kubernetes/scale-app/scale-secret.yaml
  
  # setup flask app
  kubectl apply --recursive -f kubernetes/scale-app/scale-flask.yaml

  # setup spark cluster
  kubectl apply --recursive -f kubernetes/scale-app/scale-spark.yaml

  # setup spark client
  kubectl apply --recursive -f kubernetes/scale-app/scale-spark-client.yaml

  # COMBINE: setup scale app, secrets, spark cluster
  # kubectl apply --recursive -f kubernetes/scale-app
}

load_pg () {
  # load pg files in pod
  PG_MASTER_NAME=pg-postgresql-master-0
  kubectl cp app/postgres/assets/hash_names.csv dev/${PG_MASTER_NAME}:/tmp
  kubectl cp app/postgres/assets/midi_instruments.csv dev/${PG_MASTER_NAME}:/tmp
  kubectl cp app/postgres/assets/load.sql dev/${PG_MASTER_NAME}:/tmp
  
  # kube command outside
  # kubectl exec -it ${PG_MASTER_NAME} -n dev -- psql -U postgres -f /tmp/load.sql
  
  # run in pod, two step
  # kubectl exec -it ${PG_MASTER_NAME} -n dev
  # psql -U postgres -f /tmp/load.sql
}

gremlin(){
  kubectl create namespace gremlin

  helm install gremlin gremlin/gremlin \
    --namespace gremlin \
    --set gremlin.secret.managed=true \
    --set gremlin.secret.type=secret \
    --set gremlin.secret.teamID= $GREMLIN_TEAM_ID \
    --set gremlin.secret.clusterID=test-eks-dev \
    --set gremlin.secret.teamSecret= $GREMLIN_TEAM_SECRET
}

submit_spark_job () {
  SPARK_CLIENT_POD=$(kubectl get pods --all-namespaces | grep spark-client | awk '{print $2}' | head -n 1)
  echo "spark_client_pod: ${SPARK_CLIENT_POD}"
  kubectl exec -it spark-client-8647948795-q7r5l -n dev -- /examples/scripts/run.sh run_spark_subset
}

############################
##  TEARDOWN APPLICATION  ##
############################

cleanup_scale_app () {
  # COMBINE: setup scale app, secrets, spark cluster
  # kubectl delete --recursive -f kubernetes/scale-app

  # delete spark client
  kubectl delete --recursive -f kubernetes/scale-app/scale-spark-client.yaml

  # delete spark cluster
  kubectl delete --recursive -f kubernetes/scale-app/scale-spark.yaml

  # delete flask app
  kubectl delete --recursive -f kubernetes/scale-app/scale-flask.yaml

  # delete secrets
  #kubectl delete --recursive -f kubernetes/scale-app/scale-secret.yaml
  
  # delete postgres
  helm delete --purge pg
}


"$@"
